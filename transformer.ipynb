{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLPzP80rz09fTQVROxstcr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeechXDD/9417_Pro_Project/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkcn5dNlKExu",
        "outputId": "7e8140ac-382e-44b1-e118-ab94007e1303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Z9zJBqAedo7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing"
      ],
      "metadata": {
        "id": "avhaT7ilij8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtypes={\n",
        "    'elapsed_time':np.int32,\n",
        "    'event_name':'category',\n",
        "    'name':'category',\n",
        "    'level':np.uint8,\n",
        "    'room_coor_x':np.float32,\n",
        "    'room_coor_y':np.float32,\n",
        "    'screen_coor_x':np.float32,\n",
        "    'screen_coor_y':np.float32,\n",
        "    'hover_duration':np.float32,\n",
        "    'text':'category',\n",
        "    'fqid':'category',\n",
        "    'room_fqid':'category',\n",
        "    'text_fqid':'category',\n",
        "    'fullscreen':'category',\n",
        "    'hq':'category',\n",
        "    'music':'category',\n",
        "    'level_group':'category'}"
      ],
      "metadata": {
        "id": "Zj_-q8Z7inS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_df = pd.read_csv('/content/drive/MyDrive/9417project/predict-student-performance-from-game-play.zip (Unzipped Files)/train.csv', dtype=dtypes)\n",
        "labels = pd.read_csv('/content/drive/MyDrive/9417project/predict-student-performance-from-game-play.zip (Unzipped Files)/train_labels.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccL3n10qeFze",
        "outputId": "276e0f33-428e-4f36-c622-b569449c754e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
        "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )"
      ],
      "metadata": {
        "id": "IJngK8JIk-Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
        "NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',\n",
        "        'screen_coor_x', 'screen_coor_y', 'hover_duration']"
      ],
      "metadata": {
        "id": "OZz4FdmClNsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://www.kaggle.com/code/cdeotte/random-forest-baseline-0-664/notebook\n",
        "\n",
        "def feature_engineer(dataset_df):\n",
        "    dfs = []\n",
        "    for c in CATEGORICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('nunique')\n",
        "        tmp.name = tmp.name + '_nunique'\n",
        "        dfs.append(tmp)\n",
        "    for c in NUMERICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('mean')\n",
        "        dfs.append(tmp)\n",
        "    for c in NUMERICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('std')\n",
        "        tmp.name = tmp.name + '_std'\n",
        "        dfs.append(tmp)\n",
        "    dataset_df = pd.concat(dfs,axis=1)\n",
        "    dataset_df = dataset_df.fillna(-1)\n",
        "    dataset_df = dataset_df.reset_index()\n",
        "    dataset_df = dataset_df.set_index('session_id')\n",
        "    return dataset_df"
      ],
      "metadata": {
        "id": "gWd-RU6BlcLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = feature_engineer(dataset_df)"
      ],
      "metadata": {
        "id": "UPvb1vGhlhQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, test_ratio=0.20):\n",
        "    USER_LIST = dataset.index.unique()\n",
        "    split = int(len(USER_LIST) * (1 - 0.20))\n",
        "    return dataset.loc[USER_LIST[:split]], dataset.loc[USER_LIST[split:]]\n",
        "\n",
        "train_x, valid_x = split_dataset(dataset_df)"
      ],
      "metadata": {
        "id": "REDZ7AG_ls2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the unique list of user sessions in the validation dataset. We assigned\n",
        "# `session_id` as the index of our feature engineered dataset. Hence fetching\n",
        "# the unique values in the index column will give us a list of users in the\n",
        "# validation set.\n",
        "VALID_USER_LIST = valid_x.index.unique()\n",
        "\n",
        "# Create a dataframe for storing the predictions of each question for all users\n",
        "# in the validation set.\n",
        "# For this, the required size of the data frame is:\n",
        "# (no: of users in validation set  x no of questions).\n",
        "# We will initialize all the predicted values in the data frame to zero.\n",
        "# The dataframe's index column is the user `session_id`s.\n",
        "prediction_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
        "\n",
        "# Create an empty dictionary to store the models created for each question.\n",
        "models = {}\n",
        "\n",
        "# Create an empty dictionary to store the evaluation score for each question.\n",
        "evaluation_dict ={}"
      ],
      "metadata": {
        "id": "zBjJkJrdPDE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop('label')  # Assume 'label' is your target column\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "qimjyf8FZq9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Iterate through questions 1 to 18 to train models for each question, evaluate\n",
        "# the trained model and store the predicted values.\n",
        "for q_no in range(1,19):\n",
        "  # Select level group for the question based on the q_no.\n",
        "    if q_no<=3: grp = '0-4'\n",
        "    elif q_no<=13: grp = '5-12'\n",
        "    elif q_no<=22: grp = '13-22'\n",
        "    print(\"### q_no\", q_no, \"grp\", grp)\n",
        "\n",
        "\n",
        "    # Filter the rows in the datasets based on the selected level group.\n",
        "    train_df = train_x.loc[train_x.level_group == grp]\n",
        "    train_users = train_df.index.values\n",
        "    valid_df = valid_x.loc[valid_x.level_group == grp]\n",
        "    valid_users = valid_df.index.values\n",
        "\n",
        "    # Select the labels for the related q_no.\n",
        "    train_labels = labels.loc[labels.q==q_no].set_index('session').loc[train_users]\n",
        "    valid_labels = labels.loc[labels.q==q_no].set_index('session').loc[valid_users]\n",
        "\n",
        "    # Add the label to the filtered datasets.\n",
        "    train_df[\"correct\"] = train_labels[\"correct\"]\n",
        "    valid_df[\"correct\"] = valid_labels[\"correct\"]\n",
        "\n",
        "\n",
        "    # Drop the 'correct' and 'level_group' columns from the features\n",
        "    train_features = train_df.drop(columns=[\"correct\", \"level_group\"])\n",
        "    valid_features = valid_df.drop(columns=[\"correct\", \"level_group\"])\n",
        "\n",
        "    # Convert all features to strings\n",
        "    train_features_str = train_features.astype(str)\n",
        "    valid_features_str = valid_features.astype(str)\n",
        "\n",
        "    # Join all features into a single string\n",
        "    train_sequences = train_features_str.apply(lambda x: ' '.join(x), axis=1)\n",
        "    valid_sequences = valid_features_str.apply(lambda x: ' '.join(x), axis=1)\n",
        "\n",
        "    # Encode the text sequences. This will convert the text into tokens according to the tokenizer's vocabulary, and then return the input ids and attention masks.\n",
        "    train_encodings = tokenizer(train_sequences.tolist(), truncation=True, padding=True, max_length=512)\n",
        "    valid_encodings = tokenizer(valid_sequences.tolist(), truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    # Convert our labels into numpy arrays\n",
        "    train_labels = train_df['correct'].to_numpy()\n",
        "    valid_labels = valid_df['correct'].to_numpy()\n",
        "\n",
        "    # Convert our encodings and labels into a TensorFlow Dataset object\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
        "        train_labels\n",
        "    ))\n",
        "    valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {'input_ids': valid_encodings['input_ids'], 'attention_mask': valid_encodings['attention_mask']},\n",
        "        valid_labels\n",
        "    ))\n",
        "\n",
        "    # Shuffle and batch the datasets\n",
        "    batch_size = 8\n",
        "    train_dataset = train_dataset.shuffle(100).batch(batch_size)\n",
        "    valid_dataset = valid_dataset.batch(batch_size)\n",
        "\n",
        "    # Load pre-trained model\n",
        "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Train the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_dataset, epochs=3, validation_data=valid_dataset)\n",
        "\n",
        "    models[f'{grp}_{q_no}'] = model\n",
        "\n",
        "    # Evaluate the trained model on the validation dataset\n",
        "    evaluation = model.evaluate(valid_dataset, return_dict=True)\n",
        "    evaluation_dict[q_no] = evaluation[\"accuracy\"]\n",
        "\n",
        "    # Use the trained model to make predictions on the validation dataset\n",
        "    predictions = model.predict(valid_dataset)\n",
        "    # The model returns logits, so we need to apply a softmax to get probabilities\n",
        "    predictions = tf.nn.softmax(predictions.logits, axis=-1)\n",
        "\n",
        "    # Store the predicted probabilities for the positive class in the `prediction_df` dataframe.\n",
        "    prediction_df.loc[valid_users, q_no-1] = predictions[:, 1]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dej5tydPl5Z",
        "outputId": "47d3a18c-47f0-44ba-d18e-9694b699efe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### q_no 1 grp 0-4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-58-ae34b3458d86>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df[\"correct\"] = train_labels[\"correct\"]\n",
            "<ipython-input-58-ae34b3458d86>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  valid_df[\"correct\"] = valid_labels[\"correct\"]\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd0112581f0> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_train_function.<locals>.train_function at 0x7fd0112581f0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd0112581f0> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_train_function.<locals>.train_function at 0x7fd0112581f0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "  46/2357 [..............................] - ETA: 10:24:47 - loss: 7.5881 - accuracy: 0.6168"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, value in evaluation_dict.items():\n",
        "  print(f\"question {name}: accuracy {value:.4f}\")\n",
        "\n",
        "print(\"\\nAverage accuracy\", sum(evaluation_dict.values())/18)"
      ],
      "metadata": {
        "id": "dLA4R3gwhScO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "qQfrRuwRfzWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}