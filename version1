!pip install -q kaggle
from google.colab import drive
drive.mount('/content/drive')

##change to your current directory
!mkdir -p ~/.kaggle
!cp /content/drive/My\ Drive/Colab\ Notebooks/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

from google.colab import drive
drive.mount('/content/drive')

##change to your current directory. Afterall, unzip your dataset
!kaggle competitions download -c predict-student-performance-from-game-play -p /content/drive/My\ Drive/Colab\ Notebooks/

!unzip /content/drive/My\ Drive/Colab\ Notebooks/predict-student-performance-from-game-play.zip

!pip install tensorflow_addons
!pip install tensorflow_decision_forests
!pip install tensorflow
!pip install tensorflow --upgrade
!pip install keras --upgrade

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.model_selection import KFold,GroupKFold
from sklearn.metrics import f1_score

dtypes={
    'elapsed_time':np.int32,
    'event_name':'category',
    'name':'category',
    'level':np.uint8,
    'room_coor_x':np.float32,
    'room_coor_y':np.float32,
    'screen_coor_x':np.float32,
    'screen_coor_y':np.float32,
    'hover_duration':np.float32,
    'text':'category',
    'fqid':'category',
    'room_fqid':'category',
    'text_fqid':'category',
    'fullscreen':'category',
    'hq':'category',
    'music':'category',
    'level_group':'category'}
dataset_df = pd.read_csv('train.csv', dtype=dtypes)

labels=pd.read_csv('train_labels.csv')

labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )
labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )
labels.head(3)

CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']
NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',
        'screen_coor_x', 'screen_coor_y', 'hover_duration']
# EVENTS = ['navigate_click', 'person_click', 'cutscene_click', 'object_click',
#           'map_hover', 'notification_click', 'map_click', 'observation_click',
#           'checkpoint']

# Reference: https://www.kaggle.com/code/mannubhardwaj1022/we-r-xgb-boost-model?scriptVersionId=128450896&cellId=18

def feature_engineer(dataset_df):
    dfs = []
    for c in CATEGORICAL:
        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('nunique')
        tmp.name = tmp.name + '_nunique'
        dfs.append(tmp)
    for c in NUMERICAL:
        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('mean')
        dfs.append(tmp)
    for c in NUMERICAL:
        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('std')
        tmp.name = tmp.name + '_std'
        dfs.append(tmp)
    dataset_df = pd.concat(dfs,axis=1)
    dataset_df = dataset_df.fillna(-1)
    dataset_df = dataset_df.reset_index()
    dataset_df = dataset_df.set_index('session_id')
    return dataset_df

dataset_df = feature_engineer(dataset_df)

gkf=GroupKFold(n_splits=10)
features = [c for c in dataset_df.columns if c != 'level_group']
valid=dataset_df.index.unique()
models={}
out_prediction=pd.DataFrame(data=np.zeros((len(valid),18)),index=valid)
# for i, (train_index,test_index) in enumerate(gkf.split(X=dataset_df,groups=dataset_df.index)):
#     print('#' * 25)
#     print('### Fold', i + 1)
#     print('#' * 25)


#     for t in range(1,19):
#       if t<=3:
#         grp='0-4'
#       elif t<=13:
#         grp='5-12'
#       elif t<=22:
#         grp='13-22'

#       xtrain=dataset_df.iloc[train_index]
#       xtrain=xtrain.loc[xtrain.level_group==grp]
#       train_index=xtrain.index.values
#       ytrain=labels.loc[labels.q==t].set_index('session').loc[train_index]

#       xvalid=dataset_df.iloc[test_index]
#       xvalid=xvalid.loc[xvalid.level_group==grp]
#       valid_index=xvalid.index.values
#       yvalid=labels.loc[labels.q==t].set_index('session').loc[valid_index]

#       model=XGBClassifier(**params)
#       model.fit(xtrain[features].astype('float32'),ytrain['correct'],eval_set=[(xvalid[features].astype('float32'),yvalid['correct'])],verbose=0)
#       print(f'{t}({model.best_ntree_limit}), ', end='')

#       models[f'{grp}_{t}']=model


# Perform cross-validation with 5 GroupKFold splits
for i, (train_index, test_index) in enumerate(gkf.split(X=dataset_df, groups=dataset_df.index)):
    print('#' * 25)
    print('### Fold', i + 1)
    print('#' * 25)

    params={
    'objective':'binary:logistic',
    'eval_metric':'logloss',
    'learning_rate':0.01,
    'max_depth':6,
    'n_estimators':1000,
    'early_stopping_rounds':50,
    'subsample':0.5,
    'colsample_bytree':0.5,
    }

    # Iterate through questions 1 to 18
    for t in range(1, 19):
        # Determine the level group for the current question
        if t <= 3:
            grp = '0-4'
        elif t <= 13:
            grp = '5-12'
        elif t <= 22:
            grp = '13-22'

        # Get the train and validation data for the current question and level group
        xtrain = dataset_df.iloc[train_index]
        xtrain = xtrain.loc[xtrain.level_group == grp]
        train_users = xtrain.index.values
        ytrain = labels.loc[labels.q == t].set_index('session').loc[train_users]

        xvalid = dataset_df.iloc[test_index]
        xvalid = xvalid.loc[xvalid.level_group == grp]
        valid_users = xvalid.index.values
        yvalid = labels.loc[labels.q == t].set_index('session').loc[valid_users]

        # Train an XGBoost model for the current question and level group
        model = XGBClassifier(**params)
        model.fit(
            xtrain[features].astype('float32'), ytrain['correct'],
            eval_set=[(xvalid[features].astype('float32'), yvalid['correct'])],
            verbose=0
        )
        print(f'{t}({model.best_ntree_limit}), ', end='')

        models[f'{grp}_{t}']=model

        out_prediction.loc[valid_users, t-1] = model.predict_proba(xvalid[features].astype('float32'))[:,1]

true_table=out_prediction.copy()
for i in range(18):
  tmp=labels.loc[labels.q==i+1].set_index('session').loc[valid]
  true_table[i]=tmp.correct.values

# FIND BEST THRESHOLD TO CONVERT PROBS INTO 1s AND 0s
scores = []; thresholds = []
best_score = 0; best_threshold = 0

for threshold in np.arange(0.4,0.81,0.01):
    print(f'{threshold:.02f}, ',end='')
    preds = (out_prediction.values.reshape((-1))>threshold).astype('int')
    m = f1_score(true_table.values.reshape((-1)), preds, average='macro')
    scores.append(m)
    thresholds.append(threshold)
    if m>best_score:
        best_score = m
        best_threshold = threshold

import matplotlib.pyplot as plt

# PLOT THRESHOLD VS. F1_SCORE
plt.figure(figsize=(20,5))
plt.plot(thresholds,scores,'-o',color='blue')
plt.scatter([best_threshold], [best_score], color='blue', s=300, alpha=1)
plt.xlabel('Threshold',size=14)
plt.ylabel('Validation F1 Score',size=14)
plt.title(f'Threshold vs. F1_Score with Best F1_Score = {best_score:.3f} at Best Threshold = {best_threshold:.3}',size=18)
plt.show()

print('When using optimal threshold...')
for k in range(18):

    # COMPUTE F1 SCORE PER QUESTION
    m = f1_score(true_table[k].values, (out_prediction[k].values>best_threshold).astype('int'), average='macro')
    print(f'Q{k+1}: F1 =',m)

# COMPUTE F1 SCORE OVERALL
m = f1_score(true_table.values.reshape((-1)), (out_prediction.values.reshape((-1))>best_threshold).astype('int'), average='macro')
print('==> Overall F1 =',m)

print(out_prediction)

print(true_table)
