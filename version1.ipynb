!pip install -q kaggle
from google.colab import drive
drive.mount('/content/drive')

Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).

##change to your current directory
!mkdir -p ~/.kaggle
!cp /content/drive/My\ Drive/Colab\ Notebooks/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

from google.colab import drive
drive.mount('/content/drive')
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).

##change to your current directory. Afterall, unzip your dataset
!kaggle competitions download -c predict-student-performance-from-game-play -p /content/drive/My\ Drive/Colab\ Notebooks/

!unzip /content/drive/My\ Drive/Colab\ Notebooks/predict-student-performance-from-game-play.zip
Archive:  /content/drive/My Drive/Colab Notebooks/predict-student-performance-from-game-play.zip
replace jo_wilder/__init__.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: n
replace jo_wilder/competition.cpython-37m-x86_64-linux-gnu.so? [y]es, [n]o, [A]ll, [N]one, [r]ename: n
replace jo_wilder_310/__init__.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: n
replace jo_wilder_310/competition.cpython-310-x86_64-linux-gnu.so? [y]es, [n]o, [A]ll, [N]one, [r]ename: n
replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n
replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n
replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n
replace train_labels.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n

!pip install tensorflow_addons
!pip install tensorflow_decision_forests
!pip install tensorflow
!pip install tensorflow --upgrade
!pip install keras --upgrade

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.model_selection import KFold,GroupKFold
from sklearn.metrics import f1_score
[161]
dtypes={
    'elapsed_time':np.int32,
    'event_name':'category',
    'name':'category',
    'level':np.uint8,
    'room_coor_x':np.float32,
    'room_coor_y':np.float32,
    'screen_coor_x':np.float32,
    'screen_coor_y':np.float32,
    'hover_duration':np.float32,
    'text':'category',
    'fqid':'category',
    'room_fqid':'category',
    'text_fqid':'category',
    'fullscreen':'category',
    'hq':'category',
    'music':'category',
    'level_group':'category'}
dataset_df = pd.read_csv('train.csv', dtype=dtypes)

[163]
labels=pd.read_csv('train_labels.csv')
[164]
labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )
labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )
labels.head(3)

[165]
CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']
NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',
        'screen_coor_x', 'screen_coor_y', 'hover_duration']
# EVENTS = ['navigate_click', 'person_click', 'cutscene_click', 'object_click',
#           'map_hover', 'notification_click', 'map_click', 'observation_click',
#           'checkpoint']
[166]
# Reference: https://www.kaggle.com/code/mannubhardwaj1022/we-r-xgb-boost-model?scriptVersionId=128450896&cellId=18

def feature_engineer(dataset_df):
    dfs = []
    for c in CATEGORICAL:
        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('nunique')
        tmp.name = tmp.name + '_nunique'
        dfs.append(tmp)
    for c in NUMERICAL:
        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('mean')
        dfs.append(tmp)
    for c in NUMERICAL:
        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('std')
        tmp.name = tmp.name + '_std'
        dfs.append(tmp)
    dataset_df = pd.concat(dfs,axis=1)
    dataset_df = dataset_df.fillna(-1)
    dataset_df = dataset_df.reset_index()
    dataset_df = dataset_df.set_index('session_id')
    return dataset_df

[167]
dataset_df = feature_engineer(dataset_df)
[189]
gkf=GroupKFold(n_splits=10)
features = [c for c in dataset_df.columns if c != 'level_group']
valid=dataset_df.index.unique()
models={}
out_prediction=pd.DataFrame(data=np.zeros((len(valid),18)),index=valid)
# for i, (train_index,test_index) in enumerate(gkf.split(X=dataset_df,groups=dataset_df.index)):
#     print('#' * 25)
#     print('### Fold', i + 1)
#     print('#' * 25)


#     for t in range(1,19):
#       if t<=3:
#         grp='0-4'
#       elif t<=13:
#         grp='5-12'
#       elif t<=22:
#         grp='13-22'

#       xtrain=dataset_df.iloc[train_index]
#       xtrain=xtrain.loc[xtrain.level_group==grp]
#       train_index=xtrain.index.values
#       ytrain=labels.loc[labels.q==t].set_index('session').loc[train_index]

#       xvalid=dataset_df.iloc[test_index]
#       xvalid=xvalid.loc[xvalid.level_group==grp]
#       valid_index=xvalid.index.values
#       yvalid=labels.loc[labels.q==t].set_index('session').loc[valid_index]

#       model=XGBClassifier(**params)
#       model.fit(xtrain[features].astype('float32'),ytrain['correct'],eval_set=[(xvalid[features].astype('float32'),yvalid['correct'])],verbose=0)
#       print(f'{t}({model.best_ntree_limit}), ', end='')

#       models[f'{grp}_{t}']=model


# Perform cross-validation with 5 GroupKFold splits
for i, (train_index, test_index) in enumerate(gkf.split(X=dataset_df, groups=dataset_df.index)):
    print('#' * 25)
    print('### Fold', i + 1)
    print('#' * 25)

    params={
    'objective':'binary:logistic',
    'eval_metric':'logloss',
    'learning_rate':0.01,
    'max_depth':6,
    'n_estimators':1000,
    'early_stopping_rounds':50,
    'subsample':0.5,
    'colsample_bytree':0.5,
    }

    # Iterate through questions 1 to 18
    for t in range(1, 19):
        # Determine the level group for the current question
        if t <= 3:
            grp = '0-4'
        elif t <= 13:
            grp = '5-12'
        elif t <= 22:
            grp = '13-22'

        # Get the train and validation data for the current question and level group
        xtrain = dataset_df.iloc[train_index]
        xtrain = xtrain.loc[xtrain.level_group == grp]
        train_users = xtrain.index.values
        ytrain = labels.loc[labels.q == t].set_index('session').loc[train_users]

        xvalid = dataset_df.iloc[test_index]
        xvalid = xvalid.loc[xvalid.level_group == grp]
        valid_users = xvalid.index.values
        yvalid = labels.loc[labels.q == t].set_index('session').loc[valid_users]

        # Train an XGBoost model for the current question and level group
        model = XGBClassifier(**params)
        model.fit(
            xtrain[features].astype('float32'), ytrain['correct'],
            eval_set=[(xvalid[features].astype('float32'), yvalid['correct'])],
            verbose=0
        )
        print(f'{t}({model.best_ntree_limit}), ', end='')

        models[f'{grp}_{t}']=model

        out_prediction.loc[valid_users, t-1] = model.predict_proba(xvalid[features].astype('float32'))[:,1]

#########################
### Fold 1
#########################
1(257), 2(419), 3(455), 4(453), 5(358), 6(470), 7(281), 8(284), 9(427), 10(381), 11(380), 12(466), 13(354), 14(605), 15(677), 16(327), 17(336), 18(459), #########################
### Fold 2
#########################
1(540), 2(619), 3(537), 4(411), 5(480), 6(531), 7(346), 8(332), 9(537), 10(417), 11(513), 12(467), 13(270), 14(510), 15(568), 16(395), 17(328), 18(723), #########################
### Fold 3
#########################
1(390), 2(539), 3(511), 4(715), 5(407), 6(397), 7(435), 8(268), 9(552), 10(409), 11(294), 12(312), 13(377), 14(569), 15(508), 16(325), 17(365), 18(468), #########################
### Fold 4
#########################
1(432), 2(482), 3(413), 4(620), 5(371), 6(475), 7(379), 8(239), 9(443), 10(181), 11(501), 12(369), 13(411), 14(391), 15(618), 16(330), 17(347), 18(557), #########################
### Fold 5
#########################
1(508), 2(543), 3(599), 4(539), 5(370), 6(427), 7(400), 8(365), 9(503), 10(330), 11(310), 12(415), 13(618), 14(478), 15(804), 16(244), 17(388), 18(527), #########################
### Fold 6
#########################
1(472), 2(556), 3(759), 4(884), 5(393), 6(494), 7(398), 8(391), 9(353), 10(435), 11(394), 12(459), 13(405), 14(498), 15(491), 16(302), 17(233), 18(495), #########################
### Fold 7
#########################
1(483), 2(721), 3(521), 4(606), 5(307), 6(481), 7(455), 8(158), 9(529), 10(240), 11(306), 12(431), 13(365), 14(505), 15(509), 16(421), 17(277), 18(503), #########################
### Fold 8
#########################
1(564), 2(537), 3(611), 4(570), 5(375), 6(383), 7(443), 8(289), 9(403), 10(447), 11(413), 12(385), 13(366), 14(421), 15(451), 16(382), 17(379), 18(625), #########################
### Fold 9
#########################
1(670), 2(547), 3(446), 4(549), 5(345), 6(526), 7(417), 8(299), 9(369), 10(383), 11(281), 12(550), 13(461), 14(452), 15(611), 16(420), 17(259), 18(631), #########################
### Fold 10
#########################
1(586), 2(552), 3(376), 4(611), 5(605), 6(449), 7(592), 8(295), 9(430), 10(337), 11(328), 12(457), 13(488), 14(302), 15(432), 16(342), 17(345), 18(512), 
[191]
true_table=out_prediction.copy()
for i in range(18):
  tmp=labels.loc[labels.q==i+1].set_index('session').loc[valid]
  true_table[i]=tmp.correct.values
[192]
# FIND BEST THRESHOLD TO CONVERT PROBS INTO 1s AND 0s
scores = []; thresholds = []
best_score = 0; best_threshold = 0

for threshold in np.arange(0.4,0.81,0.01):
    print(f'{threshold:.02f}, ',end='')
    preds = (out_prediction.values.reshape((-1))>threshold).astype('int')
    m = f1_score(true_table.values.reshape((-1)), preds, average='macro')
    scores.append(m)
    thresholds.append(threshold)
    if m>best_score:
        best_score = m
        best_threshold = threshold
0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 
[193]
import matplotlib.pyplot as plt

# PLOT THRESHOLD VS. F1_SCORE
plt.figure(figsize=(20,5))
plt.plot(thresholds,scores,'-o',color='blue')
plt.scatter([best_threshold], [best_score], color='blue', s=300, alpha=1)
plt.xlabel('Threshold',size=14)
plt.ylabel('Validation F1 Score',size=14)
plt.title(f'Threshold vs. F1_Score with Best F1_Score = {best_score:.3f} at Best Threshold = {best_threshold:.3}',size=18)
plt.show()

[195]
print('When using optimal threshold...')
for k in range(18):

    # COMPUTE F1 SCORE PER QUESTION
    m = f1_score(true_table[k].values, (out_prediction[k].values>best_threshold).astype('int'), average='macro')
    print(f'Q{k+1}: F1 =',m)

# COMPUTE F1 SCORE OVERALL
m = f1_score(true_table.values.reshape((-1)), (out_prediction.values.reshape((-1))>best_threshold).astype('int'), average='macro')
print('==> Overall F1 =',m)
When using optimal threshold...
Q1: F1 = 0.562659605090318
Q2: F1 = 0.49464879356568364
Q3: F1 = 0.4841949977309297
Q4: F1 = 0.5876927857656337
Q5: F1 = 0.554786619454529
Q6: F1 = 0.5967153417045693
Q7: F1 = 0.5717543241219836
Q8: F1 = 0.5244128961836775
Q9: F1 = 0.5904465742862268
Q10: F1 = 0.46475426137510306
Q11: F1 = 0.5775452000415924
Q12: F1 = 0.4800176582189724
Q13: F1 = 0.42735459613115934
Q14: F1 = 0.5954894722962971
Q15: F1 = 0.4717881134033601
Q16: F1 = 0.4562542678457517
Q17: F1 = 0.537554856771775
Q18: F1 = 0.48732565982723736
==> Overall F1 = 0.6727260371637032

[196]
print(out_prediction)
                         0         1         2         3         4         5   \
session_id                                                                      
20090312431273200  0.779464  0.986927  0.956615  0.845035  0.688855  0.846843   
20090312433251036  0.752163  0.980137  0.941633  0.571564  0.384448  0.615090   
20090312455206810  0.743615  0.978442  0.947949  0.645614  0.569263  0.866203   
20090313091715820  0.741239  0.982842  0.941179  0.780767  0.538492  0.832334   
20090313571836404  0.824519  0.986013  0.947474  0.856602  0.647115  0.875363   
...                     ...       ...       ...       ...       ...       ...   
22100215342220508  0.635932  0.979927  0.913579  0.824919  0.509368  0.734860   
22100215460321130  0.663798  0.977037  0.898011  0.739681  0.464234  0.696775   
22100217104993650  0.605076  0.974528  0.896456  0.862864  0.591119  0.820513   
22100219442786200  0.784620  0.984696  0.962326  0.795535  0.661728  0.847127   
22100221145014656  0.610576  0.954948  0.914307  0.743819  0.462606  0.632885   

                         6         7         8         9         10        11  \
session_id                                                                      
20090312431273200  0.776842  0.688605  0.814292  0.584464  0.731900  0.867942   
20090312433251036  0.643844  0.496280  0.657817  0.422861  0.523335  0.827142   
20090312455206810  0.706667  0.704424  0.806368  0.544271  0.705725  0.740989   
20090313091715820  0.742467  0.612945  0.722280  0.549469  0.650924  0.891995   
20090313571836404  0.809529  0.663288  0.814499  0.596229  0.745868  0.897352   
...                     ...       ...       ...       ...       ...       ...   
22100215342220508  0.740536  0.577894  0.757586  0.447172  0.597135  0.879721   
22100215460321130  0.656185  0.566794  0.647347  0.448160  0.630540  0.869084   
22100217104993650  0.801355  0.582133  0.802872  0.546376  0.690091  0.910106   
22100219442786200  0.765858  0.714278  0.815523  0.570340  0.777529  0.863256   
22100221145014656  0.646942  0.602441  0.673924  0.329219  0.572337  0.846964   

                         12        13        14        15        16        17  
session_id                                                                     
20090312431273200  0.425047  0.772564  0.535301  0.806556  0.759519  0.986887  
20090312433251036  0.197597  0.473228  0.150707  0.635201  0.580775  0.797444  
20090312455206810  0.490699  0.760068  0.527513  0.783154  0.776351  0.952304  
20090313091715820  0.230579  0.709052  0.545264  0.727295  0.725589  0.964230  
20090313571836404  0.390101  0.824267  0.575200  0.779301  0.753164  0.983816  
...                     ...       ...       ...       ...       ...       ...  
22100215342220508  0.199494  0.788349  0.639373  0.506986  0.595522  0.970763  
22100215460321130  0.158919  0.699669  0.524638  0.727286  0.668426  0.967968  
22100217104993650  0.179515  0.750626  0.493983  0.708735  0.644228  0.948166  
22100219442786200  0.339707  0.759762  0.509035  0.752961  0.754716  0.968950  
22100221145014656  0.200487  0.478240  0.244692  0.678408  0.571546  0.876818  

[23562 rows x 18 columns]

[197]
print(true_table)
                   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  \
session_id                                                                      
20090312431273200   1   1   1   1   1   1   1   1   1   1   1   1   0   1   1   
20090312433251036   0   1   1   1   0   1   1   0   1   0   0   1   0   1   0   
20090312455206810   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0   
20090313091715820   0   1   1   1   1   0   1   1   1   0   0   1   0   1   0   
20090313571836404   1   1   1   1   1   1   1   1   1   1   1   0   1   0   1   
...                ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..   
22100215342220508   1   1   1   1   1   1   1   0   1   1   1   1   0   1   1   
22100215460321130   0   1   1   1   0   1   1   0   1   0   1   1   0   1   0   
22100217104993650   1   1   1   1   1   1   1   1   1   0   1   1   1   1   0   
22100219442786200   0   1   1   1   1   1   1   0   1   0   1   1   0   1   0   
22100221145014656   0   1   0   1   0   0   0   0   1   0   1   1   0   0   0   

                   15  16  17  
session_id                     
20090312431273200   0   1   1  
20090312433251036   1   0   1  
20090312455206810   1   1   1  
20090313091715820   1   1   1  
20090313571836404   1   1   1  
...                ..  ..  ..  
22100215342220508   1   1   1  
22100215460321130   1   1   1  
22100217104993650   0   1   1  
22100219442786200   1   1   1  
22100221145014656   0   1   1  

[23562 rows x 18 columns]
