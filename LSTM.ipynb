{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeechXDD/9417_Pro_Project/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ueh5uId-wMpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75d91b2-0d32-4905-f2eb-0d03f628c11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.svm import SVC\n",
        "import joblib as job"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing"
      ],
      "metadata": {
        "id": "LbSLs7pXxMIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtypes={\n",
        "    'elapsed_time':np.int32,\n",
        "    'event_name':'category',\n",
        "    'name':'category',\n",
        "    'level':np.uint8,\n",
        "    'room_coor_x':np.float32,\n",
        "    'room_coor_y':np.float32,\n",
        "    'screen_coor_x':np.float32,\n",
        "    'screen_coor_y':np.float32,\n",
        "    'hover_duration':np.float32,\n",
        "    'text':'category',\n",
        "    'fqid':'category',\n",
        "    'room_fqid':'category',\n",
        "    'text_fqid':'category',\n",
        "    'fullscreen':'category',\n",
        "    'hq':'category',\n",
        "    'music':'category',\n",
        "    'level_group':'category'}"
      ],
      "metadata": {
        "id": "jLQoO_WCxMqt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_df = pd.read_csv('/content/drive/MyDrive/9417project/train.csv', dtype=dtypes)\n",
        "labels = pd.read_csv('/content/drive/MyDrive/9417project/train_labels.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGcxMlQPxOzf",
        "outputId": "9512f9c7-ab0f-468c-b1db-dd1e2f05d398"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
        "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )"
      ],
      "metadata": {
        "id": "9tPTBMuBxSuH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
        "NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',\n",
        "        'screen_coor_x', 'screen_coor_y', 'hover_duration']"
      ],
      "metadata": {
        "id": "ciIHg6HixUNA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://www.kaggle.com/code/cdeotte/random-forest-baseline-0-664/notebook\n",
        "\n",
        "def feature_engineer(dataset_df):\n",
        "    dfs = []\n",
        "    for c in CATEGORICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('nunique')\n",
        "        tmp.name = tmp.name + '_nunique'\n",
        "        dfs.append(tmp)\n",
        "    for c in NUMERICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('mean')\n",
        "        dfs.append(tmp)\n",
        "    for c in NUMERICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('std')\n",
        "        tmp.name = tmp.name + '_std'\n",
        "        dfs.append(tmp)\n",
        "    dataset_df = pd.concat(dfs,axis=1)\n",
        "    dataset_df = dataset_df.fillna(-1)\n",
        "    dataset_df = dataset_df.reset_index()\n",
        "    dataset_df = dataset_df.set_index('session_id')\n",
        "    return dataset_df"
      ],
      "metadata": {
        "id": "Xrz9SFEaxWBC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = feature_engineer(dataset_df)"
      ],
      "metadata": {
        "id": "yCavLpTFxXqP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, test_ratio=0.20):\n",
        "    USER_LIST = dataset.index.unique()\n",
        "    split = int(len(USER_LIST) * (1 - 0.20))\n",
        "    return dataset.loc[USER_LIST[:split]], dataset.loc[USER_LIST[split:]]\n",
        "\n",
        "train_x, valid_x = split_dataset(dataset_df)"
      ],
      "metadata": {
        "id": "avtUSlrWxZMX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the unique list of user sessions in the validation dataset. We assigned\n",
        "# `session_id` as the index of our feature engineered dataset. Hence fetching\n",
        "# the unique values in the index column will give us a list of users in the\n",
        "# validation set.\n",
        "VALID_USER_LIST = valid_x.index.unique()\n",
        "\n",
        "# Create a dataframe for storing the predictions of each question for all users\n",
        "# in the validation set.\n",
        "# For this, the required size of the data frame is:\n",
        "# (no: of users in validation set  x no of questions).\n",
        "# We will initialize all the predicted values in the data frame to zero.\n",
        "# The dataframe's index column is the user `session_id`s.\n",
        "prediction_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
        "SVM_prediction_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
        "# Create an empty dictionary to store the models created for each question.\n",
        "models = {}\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "# Create an empty dictionary to store the evaluation score for each question.\n",
        "evaluation_dict ={}"
      ],
      "metadata": {
        "id": "myvynSmyxu4B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop('label')  # Assume 'label' is your target column\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "_CsY1YQVxwb2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(df, sequence_length):\n",
        "    \"\"\"\n",
        "    Prepare data for LSTM, based on sequence length.\n",
        "    \"\"\"\n",
        "    # Get labels\n",
        "    labels = df.groupby(df.index)[\"correct\"].last().values\n",
        "    # Drop the 'correct' and 'level_group' columns from the features\n",
        "    features = df.drop(columns=[\"correct\", \"level_group\"])\n",
        "\n",
        "    data = []\n",
        "    label = []\n",
        "\n",
        "    # Create sequences\n",
        "    for user, group in features.groupby(features.index):\n",
        "        user_features = group.values\n",
        "        if len(user_features) < sequence_length:\n",
        "            padding = np.zeros((sequence_length - len(user_features), user_features.shape[1]))\n",
        "            user_features = np.concatenate((user_features, padding))\n",
        "        data.append(user_features)\n",
        "        label.append(labels[len(data)-1])\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    data = np.array(data)\n",
        "    label = np.array(label)\n",
        "\n",
        "    return data, label\n"
      ],
      "metadata": {
        "id": "XUoHegj2GDVg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model define\n",
        "input_shape = (100, 32)"
      ],
      "metadata": {
        "id": "x7zDRR7DyAph"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 100\n",
        "# Iterate through questions 1 to 18 to train models for each question, evaluate\n",
        "# the trained model and store the predicted values.\n",
        "for q_no in range(1,19):\n",
        "    # Select level group for the question based on the q_no.\n",
        "    if q_no<=4: grp = '0-4'\n",
        "    elif q_no<=12: grp = '5-12'\n",
        "    elif q_no<=18: grp = '13-22'\n",
        "    print(\"### q_no\", q_no, \"grp\", grp)\n",
        "\n",
        "    # Filter the rows in the datasets based on the selected level group.\n",
        "    train_df = train_x.loc[train_x.level_group == grp]\n",
        "    train_users = train_df.index.values\n",
        "    valid_df = valid_x.loc[valid_x.level_group == grp]\n",
        "    valid_users = valid_df.index.values\n",
        "\n",
        "    print(len(valid_users))\n",
        "\n",
        "    # Select the labels for the related q_no.\n",
        "    train_labels = labels.loc[labels.q==q_no].set_index('session').loc[train_users]\n",
        "    valid_labels = labels.loc[labels.q==q_no].set_index('session').loc[valid_users]\n",
        "\n",
        "    # Add the label to the filtered datasets.\n",
        "    train_df[\"correct\"] = train_labels[\"correct\"]\n",
        "    valid_df[\"correct\"] = valid_labels[\"correct\"]\n",
        "\n",
        "    # Prepare data for LSTM\n",
        "    train_data, train_labels = prepare_data(train_df, sequence_length)\n",
        "    valid_data, valid_labels = prepare_data(valid_df, sequence_length)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(len(valid_data))\n",
        "    print(len(valid_labels))\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=(sequence_length, train_data.shape[-1]), return_sequences=True))\n",
        "    model.add(LSTM(32, return_sequences=False))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_data, train_labels, epochs=3, validation_data=(valid_data, valid_labels))\n",
        "    model.save(f'/content/drive/MyDrive/9417project/models/LSTM_models/model_{grp}_{q_no}.hdf5')\n",
        "    # Store the model\n",
        "    models[f'{grp}_{q_no}'] = model\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluation = model.evaluate(valid_data, valid_labels)\n",
        "    evaluation_dict[q_no] = evaluation[1]\n",
        "    print(f\"Validation accuracy: {evaluation[1]}\")\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(valid_data)\n",
        "    prediction_df.loc[valid_users, q_no-1] = predictions.flatten()\n",
        "\n",
        "\n",
        "    binary_predictions = [1 if p > 0.62 else 0 for p in predictions]\n",
        "    # Get the true labels\n",
        "    true_labels = valid_labels[:len(valid_data)]\n",
        "\n",
        "    # Calculate the F1 score and add it to the list\n",
        "    f1 = f1_score(true_labels, binary_predictions)\n",
        "    f1_scores.append(f1)\n",
        "\n"
      ],
      "metadata": {
        "id": "iYMqGNoW0HHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, value in evaluation_dict.items():\n",
        "#   print(f\"question {name}: accuracy {value:.4f}\")\n",
        "\n",
        "# print(\"\\nAverage accuracy\", sum(evaluation_dict.values())/18)"
      ],
      "metadata": {
        "id": "Daw5PYF8GRKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "for q_no in range(1,19):\n",
        "  if q_no<=4: grp = '0-4'\n",
        "  elif q_no<=12: grp = '5-12'\n",
        "  elif q_no<=18: grp = '13-22'\n",
        "  model_path = f'/content/drive/MyDrive/9417project/models/LSTM_models/model_{grp}_{q_no}.hdf5'\n",
        "  model = load_model(model_path)\n",
        "\n",
        "\n",
        "  predictions = model.predict(valid_data)\n",
        "  prediction_df.loc[valid_users, q_no-1] = predictions.flatten()\n"
      ],
      "metadata": {
        "id": "ZdxiS0DiNuJ8",
        "outputId": "c4d0238d-3b18-4006-f550-71986cd7e42a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148/148 [==============================] - 6s 35ms/step\n",
            "148/148 [==============================] - 5s 29ms/step\n",
            "148/148 [==============================] - 10s 62ms/step\n",
            "148/148 [==============================] - 8s 49ms/step\n",
            "148/148 [==============================] - 5s 28ms/step\n",
            "148/148 [==============================] - 6s 33ms/step\n",
            "148/148 [==============================] - 5s 27ms/step\n",
            "148/148 [==============================] - 8s 35ms/step\n",
            "148/148 [==============================] - 7s 36ms/step\n",
            "148/148 [==============================] - 6s 37ms/step\n",
            "148/148 [==============================] - 5s 29ms/step\n",
            "148/148 [==============================] - 6s 34ms/step\n",
            "148/148 [==============================] - 5s 27ms/step\n",
            "148/148 [==============================] - 5s 28ms/step\n",
            "148/148 [==============================] - 5s 28ms/step\n",
            "148/148 [==============================] - 6s 34ms/step\n",
            "148/148 [==============================] - 6s 32ms/step\n",
            "148/148 [==============================] - 6s 34ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble model result\n",
        "LSTM_file_path ='/content/drive/MyDrive/9417project/predictions/LSTM_pred.npy'\n",
        "SVM_file_path ='/content/drive/MyDrive/9417project/predictions/SVM_pred.npy'\n",
        "np.save(LSTM_file_path,prediction_df)\n",
        "LSTM_pred = np.load(LSTM_file_path,allow_pickle=True)\n",
        "SVM_pred = np.load(SVM_file_path,allow_pickle=True)\n",
        "\n",
        "\n",
        "ensemble = 0.7 * SVM_pred + 0.3 * LSTM_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "2Glp-igDya-o"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YM9wAjo0Z8Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print the F1 score for each question\n",
        "# for i, score in enumerate(f1_scores, start=1):\n",
        "#     print(f\"Question {i} : F1 {score:.4f}\")\n",
        "\n",
        "# # Calculate and print the average F1 score\n",
        "# average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "# print(f\"\\nAverage F1 score: {average_f1:.4f}\")"
      ],
      "metadata": {
        "id": "VORPI9cWh7Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "max_score = 0; best_threshold = 0\n",
        "\n",
        "# Create a dataframe of required size:\n",
        "# (no: of users in validation set x no: of questions) initialized to zero values\n",
        "# to store true values of the label `correct`.\n",
        "true_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
        "for i in range(18):\n",
        "    # Get the true labels.\n",
        "    tmp = labels.loc[labels.q == i+1].set_index('session').loc[VALID_USER_LIST]\n",
        "    true_df[i] = tmp.correct.values\n",
        "\n",
        "# Loop through threshold values from 0.4 to 0.8 and select the threshold with\n",
        "# the highest `F1 score`.\n",
        "for threshold in np.arange(0.4,0.8,0.01):\n",
        "    metric = tfa.metrics.F1Score(num_classes=2,average=\"macro\",threshold=threshold)\n",
        "    y_true = tf.one_hot(true_df.values.reshape((-1)), depth=2)\n",
        "    y_pred = tf.one_hot((ensemble.reshape((-1))>threshold).astype('int'), depth=2)\n",
        "    metric.update_state(y_true, y_pred)\n",
        "    f1_score = metric.result().numpy()\n",
        "    if f1_score > max_score:\n",
        "        max_score = f1_score\n",
        "        best_threshold = threshold\n",
        "\n",
        "\n",
        "print(\"Best threshold \", best_threshold, \"\\tF1 score \", max_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI3MWtfIN4BK",
        "outputId": "72bff082-ef0b-44f8-a3b1-445fa47f3d56"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold  0.5100000000000001 \tF1 score  0.61750203\n"
          ]
        }
      ]
    }
  ]
}