{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeechXDD/9417_Pro_Project/blob/main/feature_engineer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tAb77yZ9fzMG",
        "outputId": "d268aa62-67b3-4050-f8b5-f37b13ed5af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.21.0 typeguard-2.13.3\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.25)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.11.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\n",
            "flax 0.7.0 requires jax>=0.4.2, but you have jax 0.3.25 which is incompatible.\n",
            "orbax-checkpoint 0.3.1 requires jax>=0.4.9, but you have jax 0.3.25 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\n",
            "flax 0.7.0 requires jax>=0.4.2, but you have jax 0.3.25 which is incompatible.\n",
            "orbax-checkpoint 0.3.1 requires jax>=0.4.9, but you have jax 0.3.25 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.13.1 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Collecting tensorflow_decision_forests\n",
            "  Downloading tensorflow_decision_forests-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.5.3)\n",
            "Requirement already satisfied: tensorflow~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (2.13.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (0.41.0)\n",
            "Collecting wurlitzer (from tensorflow_decision_forests)\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (3.8.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tensorflow_decision_forests) (0.32.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2022.7.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tensorflow_decision_forests) (3.2.2)\n",
            "Installing collected packages: wurlitzer, tensorflow_decision_forests\n",
            "Successfully installed tensorflow_decision_forests-1.5.0 wurlitzer-3.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install --upgrade tensorflow-addons\n",
        "!pip install keras --upgrade\n",
        "!pip install tensorflow_decision_forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pafL7Li0jyXW",
        "outputId": "8122e91c-d9ec-4607-f313-45466785667f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "myN3isf1CLXY",
        "outputId": "2f9fcca5-41ec-4e82-eadd-4ef3dc24490a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-97737db2-3be2-4e37-9236-6b5299c79189\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-97737db2-3be2-4e37-9236-6b5299c79189\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"benedictachun\",\"key\":\"a3f94529fc258924b3388f08d1efba16\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0vQV4sDCN0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18bf588-0156-4800-f0f8-fdb170525850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/root/.kaggle': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLOQK5OLCRp9",
        "outputId": "d56b0cac-7191-417c-df2c-099d750f2bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading predict-student-performance-from-game-play.zip to /content\n",
            "100% 964M/968M [00:13<00:00, 45.1MB/s]\n",
            "100% 968M/968M [00:13<00:00, 74.0MB/s]\n",
            "ref                                                         title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "----------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "nelgiriyewithana/countries-of-the-world-2023                Global Country Information Dataset 2023           23KB  2023-07-08 20:37:33           6727        239  1.0              \n",
            "alphiree/cardiovascular-diseases-risk-prediction-dataset    Cardiovascular Diseases Risk Prediction Dataset    5MB  2023-07-03 12:12:19           7206        266  1.0              \n",
            "arnavsmayan/netflix-userbase-dataset                        Netflix Userbase Dataset                          25KB  2023-07-04 07:38:41           8263        150  1.0              \n",
            "joebeachcapital/top-10000-spotify-songs-1960-now            Top 10000 Songs on Spotify 1960-Now                2MB  2023-07-26 00:54:14            803         39  1.0              \n",
            "atharvaarya25/e-commerce-analysis-uk                        E-Commerce Analysis - UK                           7MB  2023-07-26 05:25:41            721         29  1.0              \n",
            "crxxom/spotify-popular-east-asian-artists-and-tracks        Spotify Popular East Asian Artists and Tracks      1MB  2023-07-22 17:39:23            985         36  1.0              \n",
            "deependraverma13/diabetes-healthcare-comprehensive-dataset  Diabetes Healthcare: Comprehensive Dataset-AI      9KB  2023-07-23 04:24:59            578         26  1.0              \n",
            "gianinamariapetrascu/japan-life-expectancy                  üëµüèª Japan life expectancy                           1MB  2023-07-26 17:56:28            410         31  1.0              \n",
            "harishkumardatalab/housing-price-prediction                 Housing Price Prediction                           5KB  2023-07-07 04:34:24           2668         61  1.0              \n",
            "sudheerp2147234/salary-dataset-based-on-country-and-race    Salary dataset based on country and race          49KB  2023-07-06 09:10:21           1749         39  1.0              \n",
            "byomokeshsenapati/spotify-song-attributes                   Spotify Song Attributes                          883KB  2023-07-09 16:00:20           1680         48  1.0              \n",
            "gyanprakashkushwaha/realme-mobiles-dataset                  Realme Mobiles Dataset                            56KB  2023-07-26 11:06:40            352         25  1.0              \n",
            "architsharma01/loan-approval-prediction-dataset             Loan-Approval-Prediction-Dataset                  81KB  2023-07-16 16:31:20           1091         32  1.0              \n",
            "bhanupratapbiswas/zomato                                    ZomatoüåÆüçùü•ß                                          5MB  2023-07-20 08:47:17            729         34  1.0              \n",
            "bhanupratapbiswas/superstore-sales                          Superstore Sales üõíüè∑Ô∏èüõçÔ∏èüì¶üè™                         478KB  2023-07-20 09:11:47           1157         33  1.0              \n",
            "drahulsingh/list-of-shopping-malls-in-indiacsv              List-of-shopping-malls-in-India.csv                8KB  2023-07-21 17:37:15            560         30  1.0              \n",
            "bhanupratapbiswas/olympic-data                              Olympic Data ü•á ‚õ≥ü•ÖüèãÔ∏è‚Äç‚ôÄÔ∏èüö¥‚Äç‚ôÇÔ∏è                         1MB  2023-07-20 09:30:19           1332         41  1.0              \n",
            "mauryansshivam/paytm-revenue-users-transactions             Paytm Revenue, Users, Transactions                968B  2023-07-10 10:13:35            894         26  1.0              \n",
            "saloni1712/threads-an-instagram-app-reviews                 Threads, an Instagram app Reviews                  1MB  2023-07-26 10:43:24            626         31  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                       Melbourne Housing Snapshot                       451KB  2018-06-05 12:52:24         121888       1312  0.7058824        \n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c predict-student-performance-from-game-play\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9kLkhfiCTA7",
        "outputId": "a9ff7f79-c141-4f4b-f731-22efeb2616dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  predict-student-performance-from-game-play.zip\n",
            "  inflating: kaggleData/jo_wilder/__init__.py  \n",
            "  inflating: kaggleData/jo_wilder/competition.cpython-37m-x86_64-linux-gnu.so  \n",
            "  inflating: kaggleData/jo_wilder_310/__init__.py  \n",
            "  inflating: kaggleData/jo_wilder_310/competition.cpython-310-x86_64-linux-gnu.so  \n",
            "  inflating: kaggleData/sample_submission.csv  \n",
            "  inflating: kaggleData/test.csv     \n",
            "  inflating: kaggleData/train.csv    \n",
            "  inflating: kaggleData/train_labels.csv  \n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "! mkdir kaggleData\n",
        "! unzip predict-student-performance-from-game-play.zip -d kaggleData\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUAMsmXXCmBU"
      },
      "outputs": [],
      "source": [
        "# Reducing Memory Usage\n",
        "# Reference : https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65 @ARJANGROEN\n",
        "\n",
        "def get_minimal_dtype(df):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype.name\n",
        "        if ((col_type != 'datetime64[ns]') & (col_type != 'category')):\n",
        "            if (col_type != 'object'):\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "\n",
        "                if str(col_type)[:3] == 'int':\n",
        "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int8)\n",
        "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int16)\n",
        "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)\n",
        "\n",
        "                else:\n",
        "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float16)\n",
        "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        pass\n",
        "            else:\n",
        "                df[col] = df[col].astype('category')\n",
        "    mem_usg = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage became: \",mem_usg,\" MB\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDt0x92nCmsD"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('kaggleData/train.csv')\n",
        "test_data = pd.read_csv('kaggleData/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wuoQ0L-z0AW"
      },
      "outputs": [],
      "source": [
        "train_data = get_minimal_dtype(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir_J1IzXK3zK"
      },
      "outputs": [],
      "source": [
        "# Load in labels for training dataset\n",
        "labels = pd.read_csv('kaggleData/train_labels.csv')\n",
        "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
        "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjJdKp1nzTrH"
      },
      "source": [
        "#### Data Cleaning\n",
        "The columns with missing values are:\n",
        "\n",
        "- page: This is only for notebook-related events. The missing values could indicate that the event is not related to the notebook. We could fill missing values with a placeholder like -1 to denote 'Not Applicable'.\n",
        "- room_coor_x, room_coor_y, screen_coor_x, screen_coor_y: These are the coordinates of the click, and are only relevant for click events. Similar to 'page', we could fill missing values with a placeholder.\n",
        "- hover_duration: This is only for hover events. We can use the same approach as for the coordinates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLXBzjkgza29"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Find out columns with missing values\n",
        "missing_values = train_data.isnull().sum()\n",
        "# Fill missing values\n",
        "train_data['page'].fillna(-1, inplace=True)\n",
        "train_data['room_coor_x'].fillna(-1, inplace=True)\n",
        "train_data['room_coor_y'].fillna(-1, inplace=True)\n",
        "train_data['screen_coor_x'].fillna(-1, inplace=True)\n",
        "train_data['screen_coor_y'].fillna(-1, inplace=True)\n",
        "train_data['hover_duration'].fillna(-1, inplace=True)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkNC18eqpBZU"
      },
      "outputs": [],
      "source": [
        "# Create an empty DataFrame to store the correlations for each question\n",
        "correlations_by_q = pd.DataFrame()\n",
        "\n",
        "# Iterate over each question number\n",
        "for q_no in range(1, 19):\n",
        "    # Select the level group for the question based on the q_no\n",
        "    if q_no <= 4:\n",
        "        grp = '0-4'\n",
        "    elif q_no <= 12:\n",
        "        grp = '5-12'\n",
        "    else:\n",
        "        grp = '13-22'\n",
        "\n",
        "    # Filter the rows in the datasets based on the selected level group\n",
        "    filtered_data = train_data[train_data.level_group == grp]\n",
        "\n",
        "    # Select the labels for the related q_no\n",
        "    filtered_labels = labels[labels.q == q_no].set_index('session').loc[filtered_data.reset_index().session_id]\n",
        "\n",
        "    # Add the label to the filtered datasets\n",
        "    filtered_data = filtered_data.reset_index()\n",
        "    filtered_data['correct'] = filtered_labels['correct'].values\n",
        "\n",
        "    # Calculate the correlation between each feature and the target variable\n",
        "    correlations = filtered_data.drop(['session_id', 'level_group'], axis=1).corr()['correct'].sort_values(ascending=False)\n",
        "\n",
        "    # Store the correlations in the correlations_by_q DataFrame\n",
        "    correlations_by_q[q_no] = correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a_96mmKp123"
      },
      "outputs": [],
      "source": [
        "correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8OXyNAuC0u-"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trc_KfkhC-k6"
      },
      "outputs": [],
      "source": [
        "# Set plot style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Create a function for easy plotting\n",
        "def plot_count(train_data, column, title, color, rotation=0):\n",
        "    plt.figure(figsize=(12,6))\n",
        "    sns.countplot(data=train_data, x=column, order=train_data[column].value_counts().index, color=color)\n",
        "    plt.title(title, size=16)\n",
        "    plt.xticks(rotation=rotation)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjqwU2aU0DdZ"
      },
      "source": [
        "#### Distribution of the Event Names\n",
        "The most common event in the dataset is 'navigate_click', followed by 'notification_click'. These events likely relate to key interactions within the game and could be influential in a model's ability to predict student performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab8oSYGr0Bh_"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of event names\n",
        "plot_count(train_data, 'event_name', 'Distribution of Event Names', 'skyblue')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D16RPMP40Jsm"
      },
      "source": [
        "#### Distribution of Game Levels\n",
        "The distribution of game levels shows that the majority of the events are happening in the middle levels of the game (around level 10). This could suggest that most users progress to these levels before stopping, or that these levels simply have more interactive events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNU8Nha_0MAy"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of levels\n",
        "plot_count(train_data, 'level', 'Distribution of Game Levels', 'green')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na93TK9O0OtN"
      },
      "source": [
        "#### Distribution of Level Groups\n",
        "The level group distribution shows that the majority of events belong to the '5-12' level group. This is consistent with the distribution of game levels, as the majority of events occurred at these levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YqATkKg0RSA"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of level groups\n",
        "plot_count(train_data, 'level_group', 'Distribution of Level Groups', 'red')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvsx5nlPsMB3"
      },
      "source": [
        "### Interesting Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsM2ix4aahbJ"
      },
      "outputs": [],
      "source": [
        "# Analyzing 'screen_coor_x' and 'screen_coor_y'\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "sns.histplot(train_data['screen_coor_x'].dropna(), ax=ax[0], bins=30, kde=True, color='skyblue')\n",
        "ax[0].set_title('Distribution of screen_coor_x')\n",
        "\n",
        "sns.histplot(train_data['screen_coor_y'].dropna(), ax=ax[1], bins=30, kde=True, color='skyblue')\n",
        "ax[1].set_title('Distribution of screen_coor_y')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci2F_16Ez7FY"
      },
      "outputs": [],
      "source": [
        "# Scatter plot for 'screen_coor_x' and 'screen_coor_y'\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(train_data['screen_coor_x'], train_data['screen_coor_y'], alpha=0.2)\n",
        "plt.title('Scatter Plot of screen_coor_x vs screen_coor_y')\n",
        "plt.xlabel('screen_coor_x')\n",
        "plt.ylabel('screen_coor_y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def filter_data(df, column_name, values):\n",
        "    \"\"\"Filter rows where column_name is in values.\"\"\"\n",
        "    return df[df[column_name].isin(values)]\n",
        "\n",
        "def calculate_statistics(df, column_name):\n",
        "    \"\"\"Calculate mean and median of a column.\"\"\"\n",
        "    mean_val = df[column_name].mean()\n",
        "    median_val = df[column_name].median()\n",
        "    return mean_val, median_val\n",
        "\n",
        "def bin_column_data(df, column_name, bins):\n",
        "    \"\"\"Bin the column data into ranges and count the number of data points in each bin.\"\"\"\n",
        "    binned_data = pd.cut(df[column_name], bins=bins)\n",
        "    binned_counts = binned_data.value_counts().sort_index()\n",
        "    return binned_counts\n",
        "\n",
        "def correct_xticklabels(bins):\n",
        "    \"\"\"Create labels for the x-axis.\"\"\"\n",
        "    return [f'{int(bins[i])}-{int(bins[i+1])}' for i in range(len(bins)-1)]\n",
        "\n",
        "def plot_binned_counts(binned_counts, bins, mean_val, median_val, title):\n",
        "    \"\"\"Plot the binned counts.\"\"\"\n",
        "    plt.figure(figsize=(20, 9))\n",
        "    g = sns.barplot(x=binned_counts.index.astype(str), y=binned_counts.values, color='blue')\n",
        "    plt.title(title, fontsize=18)\n",
        "    g.set_xticklabels(correct_xticklabels(bins), rotation=90)\n",
        "    g.set(xlabel='hover_duration, ms', ylabel='Count')\n",
        "    g.axvline(x=np.digitize(mean_val, bins=bins)-1, color=\"red\")\n",
        "    g.text(np.digitize(mean_val, bins=bins)-1, 140000, f'Average ={round(mean_val, 1)}', rotation=90)\n",
        "    g.axvline(x=np.digitize(median_val, bins=bins)-1, color=\"red\")\n",
        "    g.text(np.digitize(median_val, bins=bins)-1, 140000, f'Median ={round(median_val, 1)}', rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "hover_data = filter_data(train_data, 'event_name', ['object_hover', 'map_hover'])\n",
        "mean_hover_duration_train, median_hover_duration_train = calculate_statistics(hover_data, 'hover_duration')\n",
        "# Define the bin edges\n",
        "bins = np.arange(0, 50001, 1000)\n",
        "hover_duration_train_binned = bin_column_data(hover_data, 'hover_duration', bins)\n",
        "\n",
        "plot_binned_counts(hover_duration_train_binned, bins, mean_hover_duration_train, median_hover_duration_train, 'hover_duration for events in train_dataset')"
      ],
      "metadata": {
        "id": "NxCAuZPBPlfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing 'elapsed_time'\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.histplot(train_data['elapsed_time'].dropna(), bins=30, kde=True, color='skyblue')\n",
        "plt.title('Distribution of elapsed_time')\n",
        "plt.xlabel('elapsed_time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L4-_mtnhTyWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdlNDNgzVpPD"
      },
      "source": [
        "#### Elapsed Time Statistics\n",
        "From the histogram of 'elapsed_time', we can observe that the distribution is heavily skewed to the right, with a few sessions having unusually high elapsed time values. These could potentially be outliers or errors in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZP4loppVq5k"
      },
      "outputs": [],
      "source": [
        "# Display statistics related to elapsed time\n",
        "elapsed_time_stats = train_data['elapsed_time'].describe()\n",
        "elapsed_time_stats\n",
        "\n",
        "# Plot the distribution of 'elapsed_time'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_data['elapsed_time'], bins=100, color='purple')\n",
        "plt.title('Distribution of Elapsed Time', size=15)\n",
        "plt.xlabel('Elapsed Time (in milliseconds)', size=11)\n",
        "plt.ylabel('Count', size=11)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl-wz1VcWADi"
      },
      "source": [
        "From the table below it shows subset of the data that falls in the top 1% of 'elapsed_time' which could suggest outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eS5tFJQWXpW"
      },
      "outputs": [],
      "source": [
        "# Check the values on the high end of 'elapsed_time'\n",
        "high_elapsed_time = train_data[train_data['elapsed_time'] > train_data['elapsed_time'].quantile(0.99)]\n",
        "high_elapsed_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RudQoqkzVutD"
      },
      "source": [
        "So I will set all 'elapsed_time' values above the 99th percentile to the 99th percentile value. This would limit the effect of extreme values without completely removing them from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCZTPg1MV6VE"
      },
      "outputs": [],
      "source": [
        "# Cap 'elapsed_time' at the 99th percentile\n",
        "train_data['elapsed_time'] = train_data['elapsed_time'].clip(upper=train_data['elapsed_time'].quantile(0.99))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96cpbCeQWM_0"
      },
      "source": [
        "The maximum value is now significantly lower than before, while the other statistics (mean, standard deviation, etc.) remain similar. This means that the extreme high values have been limited, which should help to reduce their influence on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNH_bGC6Wdcs"
      },
      "outputs": [],
      "source": [
        "# Verify the change\n",
        "train_data['elapsed_time'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yCD-5u3DTyQ"
      },
      "source": [
        "### Feature Engineering functions used in models defined below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_7xuf2CDVhK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Reference: https://www.kaggle.com/code/gusthema/student-performance-w-tensorflow-decision-forests\n",
        "\n",
        "CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
        "NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',\n",
        "        'screen_coor_x', 'screen_coor_y', 'hover_duration']\n",
        "\n",
        "def feature_engineer(dataset_df):\n",
        "    dfs = []\n",
        "    for c in CATEGORICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('nunique')\n",
        "        tmp.name = tmp.name + '_nunique'\n",
        "        dfs.append(tmp)\n",
        "    for c in NUMERICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('mean')\n",
        "        dfs.append(tmp)\n",
        "    for c in NUMERICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('std')\n",
        "        tmp.name = tmp.name + '_std'\n",
        "        dfs.append(tmp)\n",
        "    dataset_df = pd.concat(dfs,axis=1)\n",
        "    dataset_df = dataset_df.fillna(-1)\n",
        "    dataset_df = dataset_df.reset_index()\n",
        "    dataset_df = dataset_df.set_index('session_id')\n",
        "    return dataset_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Z1hMVP5AtP"
      },
      "source": [
        "### Improved version of feature engineering function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJdil45xc-BR"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
        "NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',\n",
        "        'screen_coor_x', 'screen_coor_y', 'hover_duration']\n",
        "BINNING = ['elapsed_time', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration']\n",
        "\n",
        "# Define feature engineering function\n",
        "def feature_engineer_ver2(dataset_df):\n",
        "    dfs = []\n",
        "    for c in CATEGORICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('nunique')\n",
        "        tmp.name = c + '_nunique'\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    for c in NUMERICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('mean')\n",
        "        tmp.name = c + '_mean'\n",
        "        dfs.append(tmp)\n",
        "\n",
        "        # Compute standard deviation only for certain features\n",
        "        if c in BINNING:\n",
        "            tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('std')\n",
        "            tmp.name = c + '_std'\n",
        "            dfs.append(tmp)\n",
        "\n",
        "        # Binning\n",
        "        if c in BINNING:  # Check if column is in the list of columns to bin\n",
        "            dataset_df[c+'_bin'] = pd.qcut(dataset_df[c], q=4, duplicates='drop')\n",
        "            tmp = dataset_df.groupby(['session_id','level_group'])[c+'_bin'].agg('count')\n",
        "            tmp.name = c + '_bin_count'\n",
        "            dfs.append(tmp)\n",
        "\n",
        "    # Interaction between screen coordinates\n",
        "    if 'screen_coor_x' in NUMERICAL and 'screen_coor_y' in NUMERICAL:\n",
        "        # Compute Euclidean distance instead of product\n",
        "        dataset_df['screen_coor'] = np.sqrt(dataset_df['screen_coor_x']**2 + dataset_df['screen_coor_y']**2)\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])['screen_coor'].agg(['mean', 'std'])\n",
        "        tmp.columns = ['screen_coor_mean', 'screen_coor_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    # Aggregated features\n",
        "    if 'hover_duration' in NUMERICAL:\n",
        "        dataset_df['total_hover_duration'] = dataset_df.groupby(['session_id'])['hover_duration'].transform('sum')\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])['total_hover_duration'].agg('mean')\n",
        "        tmp.name = 'total_hover_duration_mean'\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    dataset_df = pd.concat(dfs,axis=1)\n",
        "    dataset_df = dataset_df.fillna(-1)\n",
        "    dataset_df = dataset_df.reset_index()\n",
        "    dataset_df = dataset_df.set_index('session_id')\n",
        "\n",
        "    dataset_df['screen_coor_mean'] = dataset_df['screen_coor_mean'].astype('int32')\n",
        "    for col in dataset_df.select_dtypes(include='float16').columns:\n",
        "        dataset_df[col] = dataset_df[col].astype('float32')\n",
        "\n",
        "    return dataset_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBcKrSUmqn5C"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
        "NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',\n",
        "        'screen_coor_x', 'screen_coor_y', 'hover_duration']\n",
        "BINNING = ['elapsed_time', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration']\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "def feature_engineer_ver3(dataset_df):\n",
        "    dfs = []\n",
        "    pt = PowerTransformer(method='yeo-johnson')\n",
        "\n",
        "    for c in CATEGORICAL:\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('nunique')\n",
        "        tmp.name = c + '_nunique'\n",
        "        dfs.append(tmp)\n",
        "\n",
        "        # Create dummy variables for top N most common events and names\n",
        "        top_N = dataset_df[c].value_counts()[:10].index\n",
        "        for val in top_N:\n",
        "            dataset_df[c + '_' + val] = (dataset_df[c] == val).astype(int)\n",
        "        tmp = dataset_df.groupby(['session_id','level_group']).agg({c + '_' + val: 'sum' for val in top_N})\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    for c in NUMERICAL:\n",
        "        # Fill missing values with the column median\n",
        "        dataset_df[c].fillna(dataset_df[c].median(), inplace=True)\n",
        "\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('mean')\n",
        "        tmp.name = c + '_mean'\n",
        "        dfs.append(tmp)\n",
        "\n",
        "        # Compute standard deviation only for certain features\n",
        "        if c in BINNING:\n",
        "            tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('std')\n",
        "            tmp.name = c + '_std'\n",
        "            dfs.append(tmp)\n",
        "\n",
        "        # Normalize 'elapsed_time' column\n",
        "        if c == 'elapsed_time':\n",
        "            dataset_df[c] = pt.fit_transform(dataset_df[[c]])\n",
        "\n",
        "        # Binning\n",
        "        if c in BINNING:  # Check if column is in the list of columns to bin\n",
        "            dataset_df[c+'_bin'] = pd.qcut(dataset_df[c], q=4, duplicates='drop')\n",
        "            #dataset_df[c+'_bin'] = pd.qcut(dataset_df[c], q=4, duplicates='drop').astype('category')\n",
        "\n",
        "            tmp = dataset_df.groupby(['session_id','level_group'])[c+'_bin'].agg('count')\n",
        "            tmp.name = c + '_bin_count'\n",
        "            dfs.append(tmp)\n",
        "\n",
        "    # Interaction between screen coordinates\n",
        "    if 'screen_coor_x' in NUMERICAL and 'screen_coor_y' in NUMERICAL:\n",
        "        # Compute Euclidean distance instead of product\n",
        "        dataset_df['screen_coor'] = np.sqrt(dataset_df['screen_coor_x']**2 + dataset_df['screen_coor_y']**2)\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])['screen_coor'].agg(['mean', 'std'])\n",
        "        tmp.columns = ['screen_coor_mean', 'screen_coor_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    # Aggregated features\n",
        "    if 'hover_duration' in NUMERICAL:\n",
        "        dataset_df['total_hover_duration'] = dataset_df.groupby(['session_id'])['hover_duration'].transform('sum')\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])['total_hover_duration'].agg('mean')\n",
        "        tmp.name = 'total_hover_duration_mean'\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    dataset_df = pd.concat(dfs, axis=1)\n",
        "    dataset_df = dataset_df.fillna(-1)\n",
        "    dataset_df = dataset_df.reset_index()\n",
        "    dataset_df = dataset_df.set_index('session_id')\n",
        "\n",
        "    return dataset_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSRzuo6Pz_xt"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
        "NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',\n",
        "             'screen_coor_x', 'screen_coor_y', 'hover_duration']\n",
        "\n",
        "def feature_engineer_ver4(dataset_df):\n",
        "    dfs = []\n",
        "    le = LabelEncoder()\n",
        "    discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
        "\n",
        "    for c in CATEGORICAL:\n",
        "        # Label encoding for categorical features\n",
        "        dataset_df[c+'_encoded'] = le.fit_transform(dataset_df[c].astype(str))\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c+'_encoded'].agg(['mean', 'std'])\n",
        "        tmp.columns = [c + '_encoded_mean', c + '_encoded_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    for c in NUMERICAL:\n",
        "        # Fill missing values with the column median\n",
        "        dataset_df[c].fillna(dataset_df[c].median(), inplace=True)\n",
        "\n",
        "        # Calculate sum, mean and std for numerical features\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg(['sum', 'mean', 'std'])\n",
        "        tmp.columns = [c + '_sum', c + '_mean', c + '_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "        # Apply binning to numerical features\n",
        "        dataset_df[c+'_binned'] = discretizer.fit_transform(dataset_df[[c]])\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])[c+'_binned'].agg(['mean', 'std'])\n",
        "        tmp.columns = [c + '_binned_mean', c + '_binned_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    # Interaction between screen coordinates\n",
        "    if 'screen_coor_x' in NUMERICAL and 'screen_coor_y' in NUMERICAL:\n",
        "        # Compute Euclidean distance instead of product\n",
        "        dataset_df['screen_coor'] = np.sqrt(dataset_df['screen_coor_x']**2 + dataset_df['screen_coor_y']**2)\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])['screen_coor'].agg(['sum', 'mean', 'std'])\n",
        "        tmp.columns = ['screen_coor_sum', 'screen_coor_mean', 'screen_coor_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    # Aggregated features\n",
        "    if 'hover_duration' in NUMERICAL:\n",
        "        dataset_df['total_hover_duration'] = dataset_df.groupby(['session_id'])['hover_duration'].transform('sum')\n",
        "        tmp = dataset_df.groupby(['session_id','level_group'])['total_hover_duration'].agg(['mean', 'std'])\n",
        "        tmp.columns = ['total_hover_duration_mean', 'total_hover_duration_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    dataset_df = pd.concat(dfs, axis=1)\n",
        "    dataset_df = dataset_df.fillna(-1)\n",
        "    dataset_df = dataset_df.reset_index()\n",
        "    dataset_df = dataset_df.set_index('session_id')\n",
        "\n",
        "    dataset_df['page_sum'] = dataset_df['page_sum'].astype('int32')\n",
        "    for col in dataset_df.select_dtypes(include='float16').columns:\n",
        "        dataset_df[col] = dataset_df[col].astype('float32')\n",
        "\n",
        "    return dataset_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XycLQKKSDFha"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
        "NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',\n",
        "             'screen_coor_x', 'screen_coor_y', 'hover_duration']\n",
        "\n",
        "def feature_engineer_ver5(df):\n",
        "    dfs = []\n",
        "    le = LabelEncoder()\n",
        "    discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
        "\n",
        "    for c in CATEGORICAL:\n",
        "        # Label encoding for categorical features\n",
        "        df[c+'_encoded'] = le.fit_transform(df[c].astype(str))\n",
        "        tmp = df.groupby(df.index)[c+'_encoded'].agg(['mean', 'std'])\n",
        "        tmp.columns = [c + '_encoded_mean', c + '_encoded_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    for c in NUMERICAL:\n",
        "        # Fill missing values with the column median\n",
        "        df[c].fillna(df[c].median(), inplace=True)\n",
        "\n",
        "        # Calculate sum, mean and std for numerical features\n",
        "        tmp = df.groupby(df.index)[c].agg(['sum', 'mean', 'std'])\n",
        "        tmp.columns = [c + '_sum', c + '_mean', c + '_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "        # Apply binning to numerical features\n",
        "        df[c+'_binned'] = discretizer.fit_transform(df[[c]])\n",
        "        tmp = df.groupby(df.index)[c+'_binned'].agg(['mean', 'std'])\n",
        "        tmp.columns = [c + '_binned_mean', c + '_binned_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    # Interaction between screen coordinates\n",
        "    if 'screen_coor_x' in NUMERICAL and 'screen_coor_y' in NUMERICAL:\n",
        "        # Compute Euclidean distance instead of product\n",
        "        df['screen_coor'] = np.sqrt(df['screen_coor_x']**2 + df['screen_coor_y']**2)\n",
        "        tmp = df.groupby(df.index)['screen_coor'].agg(['sum', 'mean', 'std'])\n",
        "        tmp.columns = ['screen_coor_sum', 'screen_coor_mean', 'screen_coor_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    # Aggregated features\n",
        "    if 'hover_duration' in NUMERICAL:\n",
        "        df['total_hover_duration'] = df.groupby(df.index)['hover_duration'].transform('sum')\n",
        "        tmp = df.groupby(df.index)['total_hover_duration'].agg(['mean', 'std'])\n",
        "        tmp.columns = ['total_hover_duration_mean', 'total_hover_duration_std']\n",
        "        dfs.append(tmp)\n",
        "\n",
        "    df = pd.concat(dfs, axis=1)\n",
        "    df = df.fillna(-1)\n",
        "\n",
        "    df['hover_duration_sum'] = df['hover_duration_sum'].astype('int32')\n",
        "    for col in df.select_dtypes(include='float16').columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = feature_engineer(train_data)"
      ],
      "metadata": {
        "id": "I1rLtf-DC6-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QetN3gZ1Uqrq"
      },
      "outputs": [],
      "source": [
        "# Replace `inf` values:\n",
        "dataset_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Replace `NaN` values with column mean:\n",
        "dataset_df.fillna(dataset_df.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnMoEEfcR28p"
      },
      "source": [
        "### Trial Run on Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUDLJ-mql3pv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivvsKUsQmBP0"
      },
      "outputs": [],
      "source": [
        "def split_dataset(dataset, test_ratio=0.20):\n",
        "    USER_LIST = dataset.index.unique()\n",
        "    split = int(len(USER_LIST) * (1 - 0.20))\n",
        "    return dataset.loc[USER_LIST[:split]], dataset.loc[USER_LIST[split:]]\n",
        "\n",
        "train_x, valid_x = split_dataset(dataset_df)\n",
        "print(\"{} examples in training, {} examples in testing.\".format(\n",
        "    len(train_x), len(valid_x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERK5HigemEb-"
      },
      "outputs": [],
      "source": [
        "tfdf.keras.get_all_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TTcvgPDmHgE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fetch the unique list of user sessions in the validation dataset. We assigned\n",
        "# `session_id` as the index of our feature engineered dataset. Hence fetching\n",
        "# the unique values in the index column will give us a list of users in the\n",
        "# validation set.\n",
        "VALID_USER_LIST = valid_x.index.unique()\n",
        "\n",
        "# Create a dataframe for storing the predictions of each question for all users\n",
        "# in the validation set.\n",
        "# For this, the required size of the data frame is:\n",
        "# (no: of users in validation set  x no of questions).\n",
        "# We will initialize all the predicted values in the data frame to zero.\n",
        "# The dataframe's index column is the user `session_id`s.\n",
        "prediction_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
        "\n",
        "# Create an empty dictionary to store the models created for each question.\n",
        "models = {}\n",
        "\n",
        "# Create an empty dictionary to store the evaluation score for each question.\n",
        "evaluation_dict ={}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def split_dataset(dataset, test_ratio=0.20):\n",
        "    USER_LIST = dataset.index.unique()\n",
        "    split = int(len(USER_LIST) * (1 - test_ratio))\n",
        "    return dataset.loc[USER_LIST[:split]], dataset.loc[USER_LIST[split:]]\n",
        "\n",
        "# Define the feature engineering functions\n",
        "CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
        "NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',\n",
        "             'screen_coor_x', 'screen_coor_y', 'hover_duration']\n",
        "BINNING = ['elapsed_time', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration']\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer, LabelEncoder, KBinsDiscretizer\n",
        "\n",
        "# Define the feature engineering versions here (feature_engineer_ver2, feature_engineer_ver3, feature_engineer_ver4)\n",
        "\n",
        "# Apply the first feature engineering version to your training data\n",
        "dataset_df = feature_engineer(train_data)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_x, valid_x = split_dataset(dataset_df)\n",
        "print(\"{} examples in training, {} examples in testing.\".format(len(train_x), len(valid_x)))\n",
        "\n",
        "# Fetch the unique list of user sessions in the validation dataset.\n",
        "VALID_USER_LIST = valid_x.index.unique()\n",
        "\n",
        "# Create a dataframe for storing the predictions of each question for all users in the validation set.\n",
        "prediction_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
        "\n",
        "# Create an empty dictionary to store the models created for each question.\n",
        "models = {}\n",
        "\n",
        "# Create an empty dictionary to store the evaluation score for each question.\n",
        "evaluation_dict = {}\n",
        "\n",
        "models = {}  # To store the trained models\n",
        "evaluation_dict = {}  # To store the evaluation accuracies\n",
        "feature_importances = {}  # To store the feature importances\n",
        "\n",
        "# Loop over each question and train a model for it\n",
        "for q_no in range(1, 19):\n",
        "    # Select level group for the question based on the q_no.\n",
        "    if q_no<=3:\n",
        "        grp = '0-4'\n",
        "    elif q_no<=13:\n",
        "        grp = '5-12'\n",
        "    else:\n",
        "        grp = '13-22'\n",
        "\n",
        "    # Filter the rows in the datasets based on the selected level group.\n",
        "    train_df = train_x.loc[train_x.level_group == grp]\n",
        "    valid_df = valid_x.loc[valid_x.level_group == grp]\n",
        "\n",
        "    # Select the labels for the related q_no.\n",
        "    train_labels = labels.loc[labels.q==q_no].set_index('session').loc[train_df.index]\n",
        "    valid_labels = labels.loc[labels.q==q_no].set_index('session').loc[valid_df.index]\n",
        "\n",
        "    # Add the label to the filtered datasets.\n",
        "    train_df[\"correct\"] = train_labels[\"correct\"]\n",
        "    valid_df[\"correct\"] = valid_labels[\"correct\"]\n",
        "\n",
        "    # Drop the 'level_group' feature\n",
        "    train_df = train_df.drop(columns=['level_group'])\n",
        "    valid_df = valid_df.drop(columns=['level_group'])\n",
        "\n",
        "    # Split the data into features (X) and target (y)\n",
        "    X_train = train_df.drop('correct', axis=1)\n",
        "    y_train = train_df['correct']\n",
        "    X_valid = valid_df.drop('correct', axis=1)\n",
        "    y_valid = valid_df['correct']\n",
        "\n",
        "    # Train the model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Store the model\n",
        "    models[f'{grp}_{q_no}'] = model\n",
        "\n",
        "    # Get feature importances\n",
        "    importances = model.feature_importances_\n",
        "    feature_importances[q_no] = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": importances}).sort_values(\"Importance\", ascending=False)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_valid)\n",
        "    evaluation_dict[q_no] = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "# Display the feature importances and evaluations\n",
        "for q_no in range(1, 19):\n",
        "    print(f\"Question {q_no}\")\n",
        "    print(feature_importances[q_no])\n",
        "    print(f\"Evaluation accuracy: {evaluation_dict[q_no]}\")\n"
      ],
      "metadata": {
        "id": "F_E2G8N4JZSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7GEDCBVzlm8"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "models = {}  # To store the trained models\n",
        "evaluation_dict = {}  # To store the evaluation accuracies\n",
        "feature_importances = {}  # To store the feature importances\n",
        "\n",
        "for q_no in range(1, 19):\n",
        "\n",
        "    # Select level group for the question based on the q_no.\n",
        "    if q_no<=3:\n",
        "        grp = '0-4'\n",
        "    elif q_no<=13:\n",
        "        grp = '5-12'\n",
        "    else:\n",
        "        grp = '13-22'\n",
        "    print(\"### q_no\", q_no, \"grp\", grp)\n",
        "\n",
        "    # Filter the rows in the datasets based on the selected level group.\n",
        "    train_df = train_x.loc[train_x.level_group == grp]\n",
        "    train_users = train_df.index.values\n",
        "    valid_df = valid_x.loc[valid_x.level_group == grp]\n",
        "    valid_users = valid_df.index.values\n",
        "\n",
        "    # Select the labels for the related q_no.\n",
        "    train_labels = labels.loc[labels.q==q_no].set_index('session').loc[train_users]\n",
        "    valid_labels = labels.loc[labels.q==q_no].set_index('session').loc[valid_users]\n",
        "\n",
        "    # Add the label to the filtered datasets.\n",
        "    train_df[\"correct\"] = train_labels[\"correct\"]\n",
        "    valid_df[\"correct\"] = valid_labels[\"correct\"]\n",
        "\n",
        "    # Drop the 'level_group' feature\n",
        "    train_df = train_df.drop(columns=['level_group'])\n",
        "    valid_df = valid_df.drop(columns=['level_group'])\n",
        "\n",
        "    # Split your data into features (X) and target (y)\n",
        "    X_train = train_df.drop('correct', axis=1)\n",
        "    y_train = train_df['correct']\n",
        "    X_valid = valid_df.drop('correct', axis=1)\n",
        "    y_valid = valid_df['correct']\n",
        "\n",
        "    # Train the model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Store the model\n",
        "    models[f'{grp}_{q_no}'] = model\n",
        "\n",
        "    # Get feature importances\n",
        "    importances = model.feature_importances_\n",
        "\n",
        "    # Create a DataFrame for visualization\n",
        "    feature_importances[q_no] = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": importances}).sort_values(\"Importance\", ascending=False)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_valid)\n",
        "    evaluation_dict[q_no] = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "# Display the feature importances and evaluations\n",
        "for q_no in range(1, 19):\n",
        "    print(f\"Question {q_no}\")\n",
        "    print(feature_importances[q_no])\n",
        "    print(f\"Evaluation accuracy: {evaluation_dict[q_no]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}